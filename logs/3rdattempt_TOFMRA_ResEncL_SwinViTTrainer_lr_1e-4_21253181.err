wandb: Currently logged in as: hassan9000 (hassan9000-m31-biomedical-ai) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /hpf/projects/jquon/tmp/wandb/wandb/run-20251002_002312-up7xe8aq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run SwinViTTrainer_ep300_fold0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hassan9000-m31-biomedical-ai/SickKids_Dataset004_AVM_TOFMRA
wandb: üöÄ View run at https://wandb.ai/hassan9000-m31-biomedical-ai/SickKids_Dataset004_AVM_TOFMRA/runs/up7xe8aq
Traceback (most recent call last):
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/bin/nnUNetv2_train", line 7, in <module>
    sys.exit(run_training_entry())
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/run/run_training.py", line 266, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/run/run_training.py", line 207, in run_training
    nnunet_trainer.run_training()
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1371, in run_training
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/training/nnUNetTrainer/nnUNetCLSTrainer.py", line 633, in train_step
    cls_output = self.network(data)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 375, in __call__
    return super().__call__(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
    return fn(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/training/nnUNetTrainer/nnUNetCLSTrainer.py", line 382, in forward
    def forward(self, x):
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1241, in forward
    return compiled_fn(full_args)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 370, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 100, in g
    return f(*args)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/autograd/function.py", line 576, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2074, in forward
    fw_outs = call_func_at_runtime_with_args(
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 556, in wrapper
    return compiled_fn(runtime_args)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 750, in inner_fn
    outs = compiled_fn(args)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 584, in __call__
    return self.current_callable(inputs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2716, in run
    out = model(new_inputs)
  File "/hpf/projects/jquon/tmp/tmp/torchinductor_skim/yj/cyjw2efqttpuvm7osur2vf243hapjkyzmf4q6urqqzyrlpgj6ifu.py", line 15941, in call
    buf649 = empty_strided_cuda((2, 48, 160, 128, 48), (47185920, 20480, 128, 1, 983040), torch.float32)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 360.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 106.12 MiB is free. Including non-PyTorch memory, this process has 44.29 GiB memory in use. Of the allocated memory 43.75 GiB is allocated by PyTorch, and 15.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Exception in thread Exception in thread Thread-3 (results_loop):
WARNING:asyncio:socket.send() raised exception.
slurmstepd: error: *** JOB 21253181 ON cn515 CANCELLED AT 2025-10-02T00:25:00 ***
