wandb: Currently logged in as: hassan9000 (hassan9000-m31-biomedical-ai) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /hpf/projects/jquon/models/nnunetcls_baseline/tmp/wandb/wandb/run-20251001_205624-a6jwyecm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run SwinViTTrainer_ep300_fold0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hassan9000-m31-biomedical-ai/SickKids_Dataset004_AVM_TOFMRA
wandb: üöÄ View run at https://wandb.ai/hassan9000-m31-biomedical-ai/SickKids_Dataset004_AVM_TOFMRA/runs/a6jwyecm
Traceback (most recent call last):
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/bin/nnUNetv2_train", line 7, in <module>
    sys.exit(run_training_entry())
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/run/run_training.py", line 266, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/run/run_training.py", line 207, in run_training
    nnunet_trainer.run_training()
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1371, in run_training
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/hpf/projects/jquon/models/nnunetcls_baseline/nnunetv2/training/nnUNetTrainer/nnUNetCLSTrainer.py", line 633, in train_step
    cls_output = self.network(data)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 375, in __call__
    return super().__call__(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 749, in compile_wrapper
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 923, in _compile_fx_inner
    raise InductorError(e, currentframe()).with_traceback(
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 907, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1578, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1456, in codegen_and_compile
    compiled_module = graph.compile_to_module()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2293, in compile_to_module
    return self._compile_to_module()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2303, in _compile_to_module
    mod = self._compile_to_module_lines(wrapper_code)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2371, in _compile_to_module_lines
    mod = PyCodeCache.load_by_key_path(
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3296, in load_by_key_path
    mod = _reload_python_module(key, path, set_sys_modules=in_toplevel)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 31, in _reload_python_module
    exec(code, mod.__dict__, mod.__dict__)
  File "/hpf/projects/jquon/models/nnunetcls_baseline/tmp/tmp/torchinductor_skim/qx/cqxvu5kmw2wbbemd47oxq2b4vywkei5glj5vtqjmilzsfggc6xt2.py", line 14084, in <module>
    async_compile.wait(globals())
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 491, in wait
    self._wait_futures(scope)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 511, in _wait_futures
    kernel = result.result()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 4014, in result
    return self.result_fn()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 370, in get_result
    kernel, elapsed_us = task.result()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
torch._inductor.exc.InductorError: SubprocException: An exception occurred in a subprocess:

Traceback (most recent call last):
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 340, in do_job
    result = job()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 62, in _worker_compile_triton
    kernel.precompile(warm_cache_only=True)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 399, in precompile
    self._precompile_worker()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 430, in _precompile_worker
    compile_results.append(self._precompile_config(c))
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 713, in _precompile_config
    binary = triton.compile(*compile_args, **compile_kwargs)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/triton/compiler/compiler.py", line 403, in compile
    return CompiledKernel(src, metadata_group, hash)
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/triton/compiler/compiler.py", line 463, in __init__
    self.asm = AsmDict({
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/site-packages/triton/compiler/compiler.py", line 464, in <dictcomp>
    file.suffix[1:]: file.read_bytes() if file.suffix[1:] == binary_ext else file.read_text()
  File "/hpf/projects/jquon/micromamba/envs/cyy_clone/lib/python3.10/pathlib.py", line 1135, in read_text
    return f.read()
OSError: [Errno 116] Stale file handle


Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Exception in thread Exception in thread Thread-3 (results_loop)Thread-4 (results_loop)WARNING:asyncio:socket.send() raised exception.
slurmstepd: error: *** JOB 21248915 ON cn526 CANCELLED AT 2025-10-01T21:46:39 ***
